# -*- coding: utf-8 -*-
"""autolysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yR-WCwVz3vEUyqD4bdx6v8DI_twSRUa7
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from scipy.stats import ttest_ind
from sklearn.impute import SimpleImputer
import requests
import os
from textblob import TextBlob
from shapely.geometry import Point
import geopandas as gpd
import networkx as nx
from PIL import Image
import io
import base64
from sklearn.ensemble import RandomForestClassifier , IsolationForest, RandomForestRegressor
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression
from sklearn.impute import SimpleImputer

# Step 1: Load the Dataset
import pandas as pd

def load_dataset(file_path="/content/goodreads.csv"):
    try:
        print(f"Loading dataset from: {file_path}")
        data = pd.read_csv(file_path, encoding='ISO-8859-1')  # Change encoding to 'ISO-8859-1'
        print("Dataset successfully loaded.")
        return data
    except Exception as e:
        print(f"Error loading dataset: {str(e)}")
        return pd.DataFrame()



# Step 2: Explore Dataset Structure
def explore_dataset(data):
    print("\nDataset Info:")
    print(data.info())
    print("\nFirst 5 Rows:")
    print(data.head())


# Step 3: Identify Numerical and Categorical Columns
def identify_columns(data):
    numerical_cols = data.select_dtypes(include=['number']).columns.tolist()
    categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
    return numerical_cols, categorical_cols




def handle_missing_data(data):
    # Impute missing values for numerical columns with mean
    numerical_cols = data.select_dtypes(include=['number']).columns.tolist()
    imputer_num = SimpleImputer(strategy="mean")
    for col in numerical_cols:
        # Convert 2D array to 1D array
        data[col] = imputer_num.fit_transform(data[[col]]).flatten()

    # Impute missing values for categorical columns with the most frequent value
    categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
    imputer_cat = SimpleImputer(strategy="most_frequent")
    for col in categorical_cols:
        # Convert 2D array to 1D array
        data[col] = imputer_cat.fit_transform(data[[col]]).flatten()

    return data



# Outliers detection
def detect_outliers(data, numerical_cols):
    for col in numerical_cols:
        model = IsolationForest(contamination=0.05)  # 5% contamination assumed
        outliers = model.fit_predict(data[[col]])
        outliers = outliers == -1  # -1 is the label for outliers
        data[f'{col}_outlier'] = outliers
        print(f"Outliers detected in {col}: {outliers.sum()} outliers")


# Feature Importance (Using Tree-Based Models)
def feature_importance(data, target_col, numerical_cols):
    # Filter numerical columns (excluding the target)
    features = [col for col in numerical_cols if col != target_col]
    X = data[features]
    y = data[target_col]

    # Train Random Forest Classifier or Regressor based on the target type
    model = RandomForestClassifier() if y.dtype == 'object' else RandomForestRegressor()
    model.fit(X, y)

    importance = model.feature_importances_
    for feature, imp in zip(features, importance):
        print(f"Feature importance for {feature}: {imp:.4f}")


# Clustering algos like K-means or DBSCaN to group similar data
def kmeans_clustering(data, numerical_cols, n_clusters=3):
    X = data[numerical_cols].dropna()
    kmeans = KMeans(n_clusters=n_clusters)
    clusters = kmeans.fit_predict(X)

    # Add cluster labels to data
    data['cluster'] = clusters

    # Plotting the first two components (if possible)
    if len(numerical_cols) >= 2:
        plt.scatter(X[numerical_cols[0]], X[numerical_cols[1]], c=clusters, cmap='viridis')
        plt.title('K-Means Clustering')
        plt.xlabel(numerical_cols[0])
        plt.ylabel(numerical_cols[1])
        plt.show()
    else:
        print(f"Clustered data (cluster column added):\n{data[['cluster']].head()}")


# Time Series Analysis
# Time Series Analysis
def time_series_analysis(data, date_col, value_col):
    # Inspect the Date column to check for any issues
    print(f"Inspecting the first few values of the {date_col} column:")
    print(data[date_col].head())  # Print the first few values of the Date column

    try:
        # Ensure the date column is in datetime format
        data[date_col] = pd.to_datetime(data[date_col], errors='coerce')  # 'coerce' will set invalid dates to NaT
        data = data.dropna(subset=[date_col])  # Drop rows with invalid dates
    except Exception as e:
        print(f"Error converting {date_col} to datetime: {e}")
        return

    # Check if the conversion worked
    print(f"Date column after conversion: {data[date_col].head()}")

    # Set the date column as the index for time series analysis
    data.set_index(date_col, inplace=True)

    # Plot the time series
    plt.plot(data[value_col])
    plt.title(f"Time Series Analysis for {value_col}")
    plt.xlabel('Date')
    plt.ylabel(value_col)
    plt.show()


#Sentiment Analysis
def sentiment_analysis(data, text_col):
    data['sentiment'] = data[text_col].apply(lambda x: TextBlob(x).sentiment.polarity)
    print(f"Sentiment analysis results (first 5):\n{data[['sentiment']].head()}")

#Linear Regression
def linear_regression(data, numerical_cols, target_col):
    # Ensure target_col is in numerical_cols and exists in data
    if target_col not in numerical_cols or target_col not in data.columns:
        print(f"Error: Target column '{target_col}' not found in the dataset.")
        return

    X = data[numerical_cols].drop(columns=[target_col], errors='ignore').dropna()
    y = data[target_col].dropna()

    if X.empty or y.empty:
        print(f"Error: No valid data found for regression (X or y is empty).")
        return

    model = LinearRegression()
    model.fit(X, y)

    predictions = model.predict(X)

    # Plotting the regression results
    plt.scatter(X[numerical_cols[0]], y, color='blue', label='Actual')
    plt.plot(X[numerical_cols[0]], predictions, color='red', label='Predicted')
    plt.title(f"Linear Regression between {numerical_cols[0]} and {target_col}")
    plt.xlabel(numerical_cols[0])
    plt.ylabel(target_col)
    plt.legend()
    plt.show()



# Principal Component Analysis (PCA)
def perform_pca_with_insight(data, numerical_cols, api_url, api_token):
    # Perform PCA
    pca = PCA(n_components=2)
    reduced_data = pca.fit_transform(data[numerical_cols])

    # Plot PCA results
    plt.scatter(reduced_data[:, 0], reduced_data[:, 1], color="purple", alpha=0.7)
    plt.title("PCA Projection")
    plt.xlabel("Principal Component 1")
    plt.ylabel("Principal Component 2")
    plt.savefig("pca_projection.png")
    plt.close()

    # Interpret the PCA plot with LLM
    pca_image_path = "pca_projection.png"
    insights = interpret_image_with_llm(pca_image_path, api_url, api_token)

    print(f"PCA Insights: {insights}")
    return insights


# Hypothesis Testing
def hypothesis_testing(data, col1, col2):
    if col1 in data and col2 in data:
        t_stat, p_val = ttest_ind(data[col1].dropna(), data[col2].dropna())
        print(f"T-test between {col1} and {col2}: t-stat={t_stat}, p-val={p_val}")
        return t_stat, p_val
    else:
        print(f"Columns {col1} or {col2} not found for hypothesis testing.")
        return None, None


# Correlation Analysis
def correlation_with_llm(data, numerical_cols, api_url, api_token):
    # Perform Correlation Analysis
    correlation_matrix = data[numerical_cols].corr()

    # Plot Correlation Matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
    plt.title("Correlation Matrix")
    plt.savefig("correlation_matrix.png")
    plt.close()

    # Interpret the correlation matrix plot with LLM
    correlation_image_path = "correlation_matrix.png"
    insights = interpret_image_with_llm(correlation_image_path, api_url, api_token)

    print(f"Correlation Insights: {insights}")
    return insights



# Enhanced Visualizations
def visualize_data(data, numerical_cols, categorical_cols):
    for col in numerical_cols:
        plt.figure(figsize=(8, 5))
        sns.histplot(data[col], kde=True, bins=15, color="blue")
        plt.title(f"Distribution of {col}")
        plt.xlabel(col)
        plt.ylabel("Frequency")
        plt.savefig(f"{col}_distribution.png")
        plt.close()

    for col in categorical_cols:
        plt.figure(figsize=(6, 4))
        sns.countplot(x=col, data=data, palette="viridis")
        plt.title(f"Distribution of {col}")
        plt.xlabel(col)
        plt.ylabel("Count")
        plt.savefig(f"{col}_count.png")
        plt.close()

#Fund=ction to interpret an imag using LLM
  # Add this import at the top of your code

def interpret_image_with_llm(image_path, api_url, api_token):
    try:
        headers = {
            "Authorization": f"Bearer {api_token}",
            "Content-Type": "application/json"
        }

        data = {
            "model": "gpt-4o-mini",
            "messages": [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "Analyze this image content."}
            ]
        }

        response = requests.post(api_url, headers=headers, json=data)

        if response.status_code == 200:
            return response.json()
        else:
            raise Exception(f"Error {response.status_code}: {response.text}")

    except Exception as e:
        raise Exception(f"Failed to interpret image: {e}")




# Summarize Data
def summarize_data(data, numerical_cols, categorical_cols):
    summary = "Dataset Summary:\n"
    summary += f"- Number of rows: {len(data)}\n"
    summary += f"- Number of columns: {data.shape[1]}\n\n"

    for col in numerical_cols:
        summary += f"- {col}: Mean={data[col].mean():.2f}, Min={data[col].min()}, Max={data[col].max()}\n"

    for col in categorical_cols:
        summary += f"- {col}: Unique values={data[col].nunique()}, Most frequent={data[col].mode()[0]}\n"

    return summary


# LLM Analysis
def analyze_with_llm(summary, api_url, api_token):
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_token}"
    }

    payload = {
        "model": "gpt-4o-mini",
        "messages": [
            {"role": "user", "content": f"Analyze the following data summary and provide insights:\n{summary}"}
        ]
    }

    response = requests.post(api_url, headers=headers, json=payload)

    if response.status_code == 200:
        return response.json()["choices"][0]["message"]["content"]
    else:
        raise Exception(f"Error {response.status_code}: {response.text}")
#Generate Story
# Generate a narrative story
def generate_story_with_llm(summary, insights, api_url, api_token):
    story_prompt = f"""
    Create a story describing the entire data analysis process:
    1. Start with a brief introduction to the dataset received.
    2. Discuss the analysis techniques applied.
    3. Summarize the insights discovered during the process.
    4. Conclude with the implications of the findings and any recommended actions.
    Use the following as context:
    Data Summary:
    {summary}

    Insights:
    {insights}
    """
    return analyze_with_llm(story_prompt, api_url, api_token)


def save_markdown(summary, insights, story=None):
    with open("README.md", "w") as md_file:
        md_file.write("# Automated Analysis Insights\n\n")
        md_file.write("## Data Summary\n")
        md_file.write(summary + "\n\n")
        md_file.write("## LLM Insights\n")
        md_file.write(insights + "\n\n")
        if story:
            md_file.write("## Story\n")
            md_file.write(story + "\n")



# Main Function
def main():
    api_url = "https://aiproxy.sanand.workers.dev/openai/v1/chat/completions"
    api_token = input("Enter your API token: ").strip()

    # Load dataset
    file_path = "/content/goodreads.csv"
    data = load_dataset(file_path)

    if data.empty:
        print("Dataset loading failed. Exiting.")
        return

    # Handle missing data
    data = handle_missing_data(data)

    # Explore dataset
    explore_dataset(data)

    # Identify columns
    numerical_cols, categorical_cols = identify_columns(data)

    #Outliers Detection
    detect_outliers(data, numerical_cols)

    #Feature Importance (Random Forest)
    if len(numerical_cols) > 1:
        target_col = categorical_cols[0] if categorical_cols else numerical_cols[1]  # Assumes first categorical column as target if available
        feature_importance(data, target_col, numerical_cols)

    #K-means clustering
    kmeans_clustering(data, numerical_cols)


    # Time Series Analysis (if applicable)
    if 'Date' in data.columns:  # Assuming the dataset has a 'Date' column for time series analysis
        time_series_analysis(data, 'Date', numerical_cols[0])

    # Sentiment Analysis (if text column is available)
    if 'Text' in data.columns:  # Assuming the dataset has a 'Text' column for sentiment analysis
        sentiment_analysis(data, 'Text')

    #  Linear Regression (if target column exists for regression)
    if len(numerical_cols) >= 2:
        linear_regression(data, numerical_cols, target_col)


    # Perform PCA
    perform_pca_with_insight(data, numerical_cols,api_url, api_token)

    # Correlation Analysis
    correlation_with_llm(data, numerical_cols,api_url, api_token)

    # Hypothesis Testing
    if len(numerical_cols) >= 2:
        hypothesis_testing(data, numerical_cols[0], numerical_cols[1])

    # Visualize data
    visualize_data(data, numerical_cols, categorical_cols)

    # Summarize data
    summary = summarize_data(data, numerical_cols, categorical_cols)

    # Analyze with LLM
    insights = analyze_with_llm(summary, api_url, api_token)

    # Generate a story using the existing LLM function
    story = generate_story_with_llm(summary, insights, api_url, api_token)

    # Save everything to markdown
    save_markdown(summary, insights, story)
    print("Story and insights saved to README.md.")


if __name__ == "__main__":
    main()